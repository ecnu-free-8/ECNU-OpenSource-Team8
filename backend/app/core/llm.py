import json
import datetime
from flask import session
import app.core.functions as F
from openai import OpenAI
import logging

# LLMé…ç½®
LLM_CONFIGS = {
    "ecnu": {
        "base_url": "https://chat.ecnu.edu.cn/open/api/v1",
        "api_key": "sk-7520506088e9433181ebb26557dac875",
        "model": "ecnu-reasoner",
        "extra_body": {}
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "api_key": "sk-60d6d4c6bbf64ab693cf23a530b80ef3",
        "model": "qwen-plus",
        "extra_body": {"enable_thinking": True}
    }
}

# é»˜è®¤ä½¿ç”¨çš„LLM
DEFAULT_LLM = "qwen"

available_functions = {
    "get_current_datetime": F.get_current_datetime,
    "add": F.add,
    "get_summary": F.get_summary,
    "get_transactions": F.get_transactions,
    "create_transaction": F.create_transaction,
    "update_transaction": F.update_transaction,
    "delete_transaction": F.delete_transaction,
    "get_budgets": F.get_budgets,
    "create_budget": F.create_budget,
    "get_categories": F.get_categories,
    "add_category": F.add_category,
    "update_category": F.update_category,
    "delete_category": F.delete_category,
    "get_reports": F.get_reports,
}

def get_llm_client(llm_name=None):
    """è·å–LLMå®¢æˆ·ç«¯"""
    llm_name = llm_name or DEFAULT_LLM
    config = LLM_CONFIGS.get(llm_name)
    if not config:
        raise ValueError(f"æœªçŸ¥çš„LLM: {llm_name}")
    
    return OpenAI(
        api_key=config["api_key"],
        base_url=config["base_url"]
    ), config

def format_tools_for_prompt(tools):
    """ReAct format_tools_for_prompt"""
    prompt_str = """
You are a professional expense tracking assistant. Your primary task is to help users manage their expenses by accurately understanding their natural language requests and converting them into calls to the provided database operation tools (CRUD). You must strictly follow the constraints below.

### CONSTRAINTS ####
1. The tool selected must be one of the tools in the tool list.
2. When unable to find the input for the tool, please adjust immediately and use the AskHumanHelpTool to ask the user for additional parameters.
3. When you believe that you have the final answer and can respond to the user, please use the TaskCompleteTool.
4. The Thought field must be explained in chinese.
5. You must respond in JSON format, DO NOT include "```json" tags.
"""
    prompt_str += "### Partial parameter constraints ###\n"
    prompt_str += "1. å½“å‰ç”¨æˆ·åä¸º: \n"
    prompt_str += session['username'] + '\n'
    prompt_str += "2. ç°åœ¨å·²ç»è·å–æ‰€æœ‰åˆ†ç±»çš„ id ä¸åç§°ï¼Œå¦‚æœéœ€è¦ä¿®æ”¹æˆ–åˆ é™¤è¯·ä½¿ç”¨ä¸‹æ–¹çš„ id (æœ€å¤§idä¸è¡¨ç¤ºåˆ†ç±»ä¸ªæ•°)\n"
    categories = F.get_categories()['data']
    for category in categories:
        prompt_str += f"- {category}\n"
    prompt_str += "3. type å‚æ•°æœ‰ä¸‹é¢ä¸¤ç§\n"
    prompt_str += "- expense\n- income\n"
    prompt_str += "### Tool List ###\n"
    descriptions = []
    for tool_name, tool_function in tools.items():
        func_description = '\n    {'
        func_description += f"`name`: {tool_name},"
        func_description += f"`description`: {tool_function.__doc__.strip()},"
        func_description += f"`parameters`: {tool_function.__annotations__}"
        func_description += '}'
        descriptions.append(func_description)
    prompt_str += '[' + ','.join(descriptions) + '\n]'
    prompt_str += """
You should only respond in JSON format as described below

### RESPONSE FORMAT ###
    {"thought": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå·¥å…·çš„æ€è€ƒ","status": "true", "tool_names": "å·¥å…·å","args_list": {"å·¥å…·å1":{"å‚æ•°å1": "å‚æ•°å€¼1","å‚æ•°å2": "å‚æ•°å€¼2"}}}
  
    {"thought": "ç”¨æˆ·æ²¡æœ‰æä¾›å…·ä½“é—®é¢˜ï¼Œå› æ­¤éœ€è¦è¯·æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ä»¥ä¾¿é€‰æ‹©é€‚å½“çš„å·¥å…·ã€‚","tool_names": "", "status": "false","args_list": {}}

Make sure that the response content you return is all in JSON format and does not contain any extra content.
"""
    return prompt_str

def call_llm(prompt, username, functions=None, llm_name=None, use_fallback=True):
    """
    Call the LLM with a prompt and optional functions.
    
    Args:
        prompt (str): The prompt to send to the LLM.
        functions (dict, optional): A dictionary of available functions.
        llm_name (str, optional): LLM to use ('ecnu' or 'qwen')
        use_fallback (bool): Whether to try alternative LLM if primary fails
        
    Returns:
        dict: A dictionary containing the status, thought, and result of the LLM response.
    """
    llm_name = llm_name or DEFAULT_LLM
    
    try:
        client, config = get_llm_client(llm_name)
        messages = [
            {"role": "system", "content": format_tools_for_prompt(available_functions)},
            {"role": "user", "content": f"username: {username}" + prompt}
        ]
        
        # å‡†å¤‡è°ƒç”¨å‚æ•°
        call_params = {
            "model": config["model"],
            "messages": messages,
            "extra_body": config["extra_body"]
        }
        
        response = client.chat.completions.create(**call_params)
        logging.debug("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”LLM Responseâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
        logging.debug(response.choices[0].message.content)
        
        try:
            json_response = json.loads(response.choices[0].message.content)
        except json.JSONDecodeError as e:
            logging.error(f"JSONè§£æå¤±è´¥: {str(e)}")
            logging.error(f"LLMåŸå§‹å“åº”: {response.choices[0].message.content}")
            return {'status': False}
            
        if json_response.get("status", "false") == "false":
            return {'status': False}
        thought = json_response.get("thought", "")
        function_name = json_response.get("tool_names", None)
        args = json_response.get("args_list", {})
        args = args.get(function_name, {}) if function_name else {}
        function = functions.get(function_name) if function_name else None
        
        try:
            result = function(**args) if function else None
        except Exception as e:
            logging.error(f"å‡½æ•°è°ƒç”¨å¤±è´¥: {function_name}, å‚æ•°: {args}, é”™è¯¯: {str(e)}")
            return {'status': False}
            
        return {
            'status': True,
            'thought': thought,
            'result': result,
        }
        
    except Exception as e:
        logging.error(f"LLMè°ƒç”¨å¼‚å¸¸ ({llm_name}): {str(e)}")
        
        # å¦‚æœå¯ç”¨äº†fallbackä¸”å½“å‰ä¸æ˜¯å¤‡ç”¨LLMï¼Œå°è¯•å¤‡ç”¨LLM
        fallback_llm = "qwen" if llm_name == "ecnu" else "ecnu"
        if use_fallback and llm_name != fallback_llm:
            logging.info(f"å°è¯•ä½¿ç”¨å¤‡ç”¨LLM: {fallback_llm}")
            return call_llm(prompt, username, functions, fallback_llm, use_fallback=False)
        
        return {'status': False}

def chat_llm(username, prompt, llm_name=None, use_fallback=True):
    """
    è¿™æ˜¯ä¸€ä¸ªäºŒé˜¶æ®µçš„è°ƒç”¨å‡½æ•°ï¼Œé¦–å…ˆè°ƒç”¨call_llmå‡½æ•°è·å–ç»“æœï¼š
    å¦‚æœç»“æœçŠ¶æ€ä¸ºFalseï¼Œåˆ™ç›´æ¥ä½¿ç”¨OpenAIçš„APIè¿›è¡ŒèŠå¤©å›å¤ã€‚
    å¦‚æœç»“æœçŠ¶æ€ä¸ºTrueï¼Œåˆ™æ ¹æ®è°ƒç”¨å‡½æ•°åçš„ç»“æœä½¿ç”¨OpenAIçš„APIè¿›è¡ŒèŠå¤©å›å¤ã€‚
    
    Args:
        prompt (str): The prompt to send to the LLM.
        username (str): The username of the user making the request.
        llm_name (str, optional): LLM to use ('ecnu' or 'qwen')
        use_fallback (bool): Whether to try alternative LLM if primary fails
    Returns:
        dict: A dictionary containing the status and data of the response.
    """
    llm_name = llm_name or DEFAULT_LLM
    
    try:
        result = call_llm(prompt, username, functions=available_functions, 
                         llm_name=llm_name, use_fallback=use_fallback)
        
        client, config = get_llm_client(llm_name)
        
        if result['status'] == False:
            try:
                messages = [
                    {"role": "system", "content": "You are a professional expense tracking assistant."},
                    {"role": "user", "content": prompt}
                ]
                call_params = {
                    "model": config["model"],
                    "messages": messages,
                    "extra_body": config["extra_body"]
                }
                response = client.chat.completions.create(**call_params)
                return {
                    "success": True,
                    "data": response.choices[0].message.content.strip()
                }
            except Exception as e:
                logging.error(f"ç›´æ¥èŠå¤©è°ƒç”¨å¤±è´¥ ({llm_name}): {str(e)}")
                
                # å°è¯•å¤‡ç”¨LLM
                fallback_llm = "qwen" if llm_name == "ecnu" else "ecnu"
                if use_fallback and llm_name != fallback_llm:
                    logging.info(f"å°è¯•ä½¿ç”¨å¤‡ç”¨LLMè¿›è¡ŒèŠå¤©: {fallback_llm}")
                    return chat_llm(username, prompt, fallback_llm, use_fallback=False)
                
                return {
                    "success": False,
                    "data": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"
                }
        
        try:
            messages = [
                {"role": "system", "content": "You are a professional expense tracking assistant. Please respond to the user's request based on the function call result."},
                {"role": "user", "content": "ç”¨æˆ·è¯·æ±‚ä¸ºï¼š\n" + prompt + f"""

æ ¹æ®ç”¨æˆ·è¯·æ±‚è°ƒç”¨å‡½æ•°åç»“æœä¸ºï¼š\n"+ {json.dumps(result, ensure_ascii=False)} + "\n\nè¯·ç»™ä¸€ä¸ªåˆé€‚çš„å›åº”æ¯”å¦‚ï¼š
"å¥½çš„ï¼Œæˆ‘å·²ç»è®°å½•äº†è¿™ç¬”å¼€æ”¯ï¼š\nğŸ’° é‡‘é¢ï¼š25å…ƒ\nğŸ½ åˆ†ç±»ï¼šé¤é¥®\nğŸ“… æ—¶é—´ï¼šä»Šå¤©";
"æœ¬æœˆæ‚¨æ€»å…±èŠ±è´¹äº†2580.00å…ƒï¼Œä¸»è¦æ”¯å‡ºä¸ºé¤é¥®580å…ƒï¼Œäº¤é€š320å…ƒã€‚";
"""},
            ]
            logging.debug(f"ç¬¬äºŒé˜¶æ®µLLMè°ƒç”¨ - ç”¨æˆ·: {username}, å‡½æ•°ç»“æœ: {json.dumps(result, ensure_ascii=False)}")
            
            call_params = {
                "model": config["model"],
                "messages": messages,
                "extra_body": config["extra_body"]
            }
            response = client.chat.completions.create(**call_params)
            logging.debug(f"ç¬¬äºŒé˜¶æ®µLLMå“åº”: {response.choices[0].message.content}")
            return {
                "success": True,
                "data": response.choices[0].message.content.strip()
            }
        except Exception as e:
            logging.error(f"å‡½æ•°ç»“æœå¤„ç†è°ƒç”¨å¤±è´¥ ({llm_name}): {str(e)}")
            
            # å°è¯•å¤‡ç”¨LLM
            fallback_llm = "qwen" if llm_name == "ecnu" else "ecnu"
            if use_fallback and llm_name != fallback_llm:
                logging.info(f"ç¬¬äºŒé˜¶æ®µå°è¯•ä½¿ç”¨å¤‡ç”¨LLM: {fallback_llm}")
                return chat_llm(username, prompt, fallback_llm, use_fallback=False)
            
            return {
                "success": False,
                "data": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"
            }
    except Exception as e:
        logging.error(f"chat_llmæ•´ä½“è°ƒç”¨å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "data": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"
        }

# ä¾¿åˆ©å‡½æ•°
def set_default_llm(llm_name):
    """è®¾ç½®é»˜è®¤LLM"""
    global DEFAULT_LLM
    if llm_name in LLM_CONFIGS:
        DEFAULT_LLM = llm_name
        logging.info(f"é»˜è®¤LLMå·²è®¾ç½®ä¸º: {llm_name}")
    else:
        raise ValueError(f"æœªçŸ¥çš„LLM: {llm_name}")

def get_available_llms():
    """è·å–å¯ç”¨çš„LLMåˆ—è¡¨"""
    return list(LLM_CONFIGS.keys())
    
if __name__ == "__main__":
    # Example usage
    print("=== ä½¿ç”¨é»˜è®¤LLM (ECNU) ===")
    print(call_llm("è®¡ç®— 329485234587 å’Œ 23985789234 çš„å’Œ", "user1", functions=available_functions))
    print(chat_llm("user1", "è®¡ç®— 329485234587 å’Œ 23985789234 çš„å’Œ"))
    
    print("\n=== ä½¿ç”¨Qwen LLM ===")
    print(chat_llm("user1", "ä½ å¥½", llm_name="qwen"))
    
    print("\n=== åˆ‡æ¢é»˜è®¤LLMåˆ°Qwen ===")
    set_default_llm("qwen")
    print(chat_llm("user1", "ä½ å¥½"))
    
    print(f"\n=== å¯ç”¨çš„LLM: {get_available_llms()} ===")
    
    # å› ä¸ºæ¶‰åŠåˆ°Flaskä¸Šä¸‹æ–‡ï¼Œ æ¶‰åŠåˆ°æ•°æ®åº“æŸ¥è¯¢çš„åªèƒ½é€šè¿‡apiè°ƒç”¨æ¥æµ‹è¯•