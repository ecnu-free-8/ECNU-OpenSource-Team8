import json
import datetime
import app.core.functions as F
from openai import OpenAI

available_functions = {
    "get_current_datetime": F.get_current_datetime,
    "add": F.add,
    "get_summary": F.get_summary,
    "get_transactions": F.get_transactions,
    "create_transaction": F.create_transaction,
    "update_transaction": F.update_transaction,
    "delete_transaction": F.delete_transaction,
    "get_budgets": F.get_budgets,
    "create_budget": F.create_budget,
    "get_categories": F.get_categories,
    "add_category": F.add_category,
    "update_category": F.update_category,
    "delete_category": F.delete_category,
    "get_reports": F.get_reports,
}

# for name, func in available_functions.items():
#     print(name)
#     print(func.__doc__)
#     print('â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”')

url = 'https://chat.ecnu.edu.cn/open/api/v1'
api_key = 'sk-7520506088e9433181ebb26557dac875'

def format_tools_for_prompt(tools): # ReAct format_tools_for_prompt
    prompt_str = """
You are a professional expense tracking assistant. Your primary task is to help users manage their expenses by accurately understanding their natural language requests and converting them into calls to the provided database operation tools (CRUD). You must strictly follow the constraints below.

### CONSTRAINTS ####
1. The tool selected must be one of the tools in the tool list.
2. When unable to find the input for the tool, please adjust immediately and use the AskHumanHelpTool to ask the user for additional parameters.
3. When you believe that you have the final answer and can respond to the user, please use the TaskCompleteTool.
4. The Thought field must be explained in chinese.
5. You must respond in JSON format, DO NOT include "```json" tags.
"""
    prompt_str += "### Partial parameter constraints ###\n"
    prompt_str += "1. ç°åœ¨å·²ç»è·å–æ‰€æœ‰åˆ†ç±»çš„ id ä¸åç§°ï¼Œå¦‚æœéœ€è¦ä¿®æ”¹æˆ–åˆ é™¤è¯·ä½¿ç”¨ä¸‹æ–¹çš„ id (æœ€å¤§idä¸è¡¨ç¤ºåˆ†ç±»ä¸ªæ•°)\n"
    categories = F.get_categories()['data']
    for category in categories:
        prompt_str += f"- {category}\n"
    prompt_str += "2. type å‚æ•°æœ‰ä¸‹é¢ä¸¤ç§\n"
    prompt_str += "- expense\n- income\n"
    prompt_str += "### Tool List ###\n"
    descriptions = []
    for tool_name, tool_function in tools.items():
        func_description = '\n    {'
        func_description += f"`name`: {tool_name},"
        func_description += f"`description`: {tool_function.__doc__.strip()},"
        func_description += f"`parameters`: {tool_function.__annotations__}"
        func_description += '}'
        descriptions.append(func_description)
    prompt_str += '[' + ','.join(descriptions) + '\n]'
    prompt_str += """
You should only respond in JSON format as described below

### RESPONSE FORMAT ###
    {"thought": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå·¥å…·çš„æ€è€ƒ","status": "true", "tool_names": "å·¥å…·å","args_list": {â€œå·¥å…·å1â€:{"å‚æ•°å1": "å‚æ•°å€¼1","å‚æ•°å2": "å‚æ•°å€¼2"}}}
  
    {"thought": "ç”¨æˆ·æ²¡æœ‰æä¾›å…·ä½“é—®é¢˜ï¼Œå› æ­¤éœ€è¦è¯·æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ä»¥ä¾¿é€‰æ‹©é€‚å½“çš„å·¥å…·ã€‚","tool_names": "", "status": "false","args_list": {}}

Make sure that the response content you return is all in JSON format and does not contain any extra content.
"""
    return prompt_str

def call_llm(prompt, username, functions=None):
    """
    Call the LLM with a prompt and optional functions.
    
    Args:
        prompt (str): The prompt to send to the LLM.
        functions (dict, optional): A dictionary of available functions.
        
    Returns:
        dict: A dictionary containing the status, thought, and result of the LLM response.
    """
    
    client = OpenAI(api_key=api_key, base_url=url)
    messages = [
        {"role": "system", "content": format_tools_for_prompt(available_functions)},
        {"role": "user", "content": f"username: {username}" + prompt}
    ]
    # print(messages[0]['content'])
    response = client.chat.completions.create(
        model="ecnu-reasoner",
        messages=messages,
    )
    import logging
    logging.debug("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”LLM Responseâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    logging.debug(response.choices[0].message.content)  # Debugging output
    json_response = json.loads(response.choices[0].message.content)
    if json_response.get("status", "false") == "false":
        return {'status': False}
    thought = json_response.get("thought", "")
    function_name = json_response.get("tool_names", None)
    args = json_response.get("args_list", {})
    args = args.get(function_name, {}) if function_name else {}
    function = functions.get(function_name) if function_name else None
    result = function(**args) if function else None
    return {
        'status': True,
        'thought': thought,
        'result': result,
    }

def chat_llm(username, prompt):
    """
    è¿™æ˜¯ä¸€ä¸ªäºŒé˜¶æ®µçš„è°ƒç”¨å‡½æ•°ï¼Œé¦–å…ˆè°ƒç”¨call_llmå‡½æ•°è·å–ç»“æœï¼š
    å¦‚æœç»“æœçŠ¶æ€ä¸ºFalseï¼Œåˆ™ç›´æ¥ä½¿ç”¨OpenAIçš„APIè¿›è¡ŒèŠå¤©å›å¤ã€‚
    å¦‚æœç»“æœçŠ¶æ€ä¸ºTrueï¼Œåˆ™æ ¹æ®è°ƒç”¨å‡½æ•°åçš„ç»“æœä½¿ç”¨OpenAIçš„APIè¿›è¡ŒèŠå¤©å›å¤ã€‚
    
    Args:
        prompt (str): The prompt to send to the LLM.
        username (str): The username of the user making the request.
    Returns:
        dict: A dictionary containing the status and data of the response.
    """
    result = call_llm(prompt, username, functions=available_functions)
    
    client = OpenAI(api_key=api_key, base_url=url)
    if result['status'] == False:
        messages = [
            {"role": "system", "content": "You are a professional expense tracking assistant."},
            {"role": "user", "content": prompt}
        ]
        response = client.chat.completions.create(
            model="ecnu-reasoner",
            messages=messages,
        )
        return {
            "success": True,
            "data": response.choices[0].message.content.strip()
        }
    
    messages = [
        {"role": "system", "content": "You are a professional expense tracking assistant. Please respond to the user's request based on the function call result."},
        {"role": "user", "content": "ç”¨æˆ·è¯·æ±‚ä¸ºï¼š\n" + prompt + f"""

æ ¹æ®ç”¨æˆ·è¯·æ±‚è°ƒç”¨å‡½æ•°åç»“æœä¸ºï¼š\n"+ {json.dumps(result, ensure_ascii=False)} + "\n\nè¯·ç»™ä¸€ä¸ªåˆé€‚çš„å›åº”æ¯”å¦‚ï¼š
"å¥½çš„ï¼Œæˆ‘å·²ç»è®°å½•äº†è¿™ç¬”å¼€æ”¯ï¼š\nğŸ’° é‡‘é¢ï¼š25å…ƒ\nğŸ½ åˆ†ç±»ï¼šé¤é¥®\nğŸ“… æ—¶é—´ï¼šä»Šå¤©";
"æœ¬æœˆæ‚¨æ€»å…±èŠ±è´¹äº†2580.00å…ƒï¼Œä¸»è¦æ”¯å‡ºä¸ºé¤é¥®580å…ƒï¼Œäº¤é€š320å…ƒã€‚";
"""},
    ]
    response = client.chat.completions.create(
        model="ecnu-reasoner",
        messages=messages,
    )
    return {
        "success": True,
        "data": response.choices[0].message.content.strip()
    }
    
if __name__ == "__main__":
    # Example usage
    print(call_llm("è®¡ç®— 329485234587 å’Œ 23985789234 çš„å’Œ", "user1", functions=available_functions))
    print(chat_llm("è®¡ç®— 329485234587 å’Œ 23985789234 çš„å’Œ", "user1"))
    # å› ä¸ºæ¶‰åŠåˆ°Flaskä¸Šä¸‹æ–‡ï¼Œ æ¶‰åŠåˆ°æ•°æ®åº“æŸ¥è¯¢çš„åªèƒ½é€šè¿‡apiè°ƒç”¨æ¥æµ‹è¯•